{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12f48f9",
   "metadata": {},
   "source": [
    "### Rodrigo GonzÃ¡lez Linares\n",
    "\n",
    "# Language Modeling: text generation\n",
    "\n",
    "`LM.py` includes two classes, `ToyLM_ngram` and `ToyLM_LSTM`. The former implements an n-gram model, while the latter implements an Long Short-Term Memory (LSTM) network-based model; both for the purpose of text generation.  \n",
    "First of all it is necessary to import the `LM.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f4c645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 00:48:59.537145: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-05 00:48:59.537237: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import LM as lm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a791c6c",
   "metadata": {},
   "source": [
    "## n-gram models \n",
    "### Queen of England 3-gram model\n",
    "Next we need to create an object for the `ToyLM_ngram` class. As this first model will be trained on Queen Elizabeth II's speeches under a 3-gram schema, we will appropriately name it `lizzie_3gram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12421bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lizzie_3gram = lm.ToyLM_ngram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5b46b",
   "metadata": {},
   "source": [
    "Before creating the model, we need to process the raw text from the speeches to get the train and test sets. This can be done using the `GetSentences` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475218c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = lizzie_3gram.GetSentences('./materials/HerMajestySpeechesDataset/train.txt')\n",
    "testSentences = lizzie_3gram.GetSentences('./materials/HerMajestySpeechesDataset/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc004ce2",
   "metadata": {},
   "source": [
    "Once that is done we create the model, indicating the value of `n`, using `CreateModel`. We can also save the model with `Save` indicating the path where it should be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b02f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lizzie_3gram.CreateModel(trainSentences,3)\n",
    "lizzie_3gram.Save('models/lizzie_3gram.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f896812b",
   "metadata": {},
   "source": [
    "When this is done we can calculate the average perplexity for the test set. If a model produces a perplexity `x`, this roughly equates to the model being as confused as trying to choose from `x` random tokens. Then, the lower the perplexity the better. As a rule of thumb, if the vocabulary size is much larger than perplexity, then the model is performing well.  \n",
    "There is a big caveat regarding perplexity and how the n-gram model was set up. If a given m-gram (where m=n-1) is not contained within the corpus, then the model will generate the next token piking the last token of any n-gram in the corpus, based on its total number of counts (i.e. the higher the count the most probable the last token of a particular n-gram will be picked as the next token to be generated). This is a palliative mechanism to avoid the model halting. If this happen and the ground truth label is not present in the corpus, with regards to perplexity these are deemed as \"invalid tests\". On the other hand, if the label is contained within a corpus these will be regarded as \"valid tests\". The perplexity is only calculated for valid tests, as the model would be \"infinitely perplex\" by tokens not present in the vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33efb033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average perplexity is:  8.099249\n",
      "* Note: The percentage of valid tests is:  25.41532573714882 %\n"
     ]
    }
   ],
   "source": [
    "lizzie_3gram.Test(testSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989268a8",
   "metadata": {},
   "source": [
    "At this point we are ready to predict. We can do this using the `Predict` method, specifying the initial context and the maximum number of tokens to be generated. The generation will stop when either the maximum number of tokens have been generated (30 for all the examples in the notebook), or when an end-of-sentence symbol is reached. The dot (`.`) has been defined as the end-of-sentence symbol.  \n",
    "During the text processing all words were uncapitalized; so all capitalized words will surely not be contained in any n-gram. It is therefore recommended that every word in the provided context is uncapitalized.  \n",
    "We will start by predicting using only the most probable n-gram by setting `numberOfConsideredWords` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fe929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 2 2 , 0 0 2 2 .\n",
      " \n",
      "hello , i am the very first visit fifty - two years ago , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth\n",
      " \n",
      "i love the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth\n",
      " \n",
      "where are you philip 0 0 2 2 , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth\n",
      " \n",
      "hi there folks 0 0 2 2 .\n",
      " \n",
      "a gin for me , this is a pleasure to be back in australia and all those who have been affected by yesterday ' s christmas broadcast 2 0 0 2 2\n",
      " \n",
      " ireland is 0 0 2 2 , when i was last in australia and britain stood side by side in two of the commonwealth , and the commonwealth , and\n",
      " \n",
      " the scottich 0 0 2 2 , when i addressed the theme of the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth , and the\n",
      " \n",
      "i look at our country .\n",
      " \n",
      "the houses of parliament democracy 0 0 2 2 , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth , and the commonwealth , and the\n",
      " \n"
     ]
    }
   ],
   "source": [
    "contexts = ['',\n",
    "           'hello, i am the',\n",
    "           'i love the commonwealth',\n",
    "           'where are you philip',\n",
    "           'hi there folks',\n",
    "           'a gin for me',\n",
    "           'ireland is',\n",
    "           'the scottich',\n",
    "           'i look at our country',\n",
    "            'the houses of parliament']\n",
    "\n",
    "for context in contexts:\n",
    "    print(lizzie_3gram.Predict(context,maxLength=30,numberOfConsideredWords=1))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e404c5db",
   "metadata": {},
   "source": [
    "Next we use all the available n-grams for prediction based on its probability distribution (which is the default).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e089ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gentlemen thank of on world history .\n",
      " \n",
      "hello , i am the very welcome progress in our national character .\n",
      " \n",
      "i love the commonwealth - canada , 1 1 march 2 0 0 2 2 is not a year of the diversity that has necessarily kept people apart has , quite simply\n",
      " \n",
      "where are you philip more , a vivid passage in her diary the first time , and cambridge have all been affected continue to encourage our young people - safe , environmentally\n",
      " \n",
      "hi there folks games coronation golden it from states in from its citizens from every background and experience .\n",
      " \n",
      "a gin for me this year , and saint paul reminded parents to be done , soon be reunited with their families .\n",
      " \n",
      " ireland is , of which you fulfil .\n",
      " \n",
      " the scottich ' members our that of your homeland as the regiment during those years ago , he lost friends in the northern irish countryside .\n",
      " \n",
      "i look at our country .\n",
      " \n",
      "the houses of parliament .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(lizzie_3gram.Predict(context,maxLength=30))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce452741",
   "metadata": {},
   "source": [
    "Finally, we will set a cutoff and only consider the 50 top n-grams, and then make a choice, again, given their probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c3de97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  i will open my home .\n",
      " \n",
      "hello , i am the very first broadcast i made my first christmas broadcast 1 9 4 0 0 2 2 , and now the commonwealth , as we continue to offer my\n",
      " \n",
      "i love the commonwealth treasures and respects this wealth of natural resources with greater care , professionalism and sensitivity often in places that are now among the members of my accession with\n",
      " \n",
      "where are you philip and me a right and proper way of describing this parliament to give care to those who died in the structure of society .\n",
      " \n",
      "hi there folks you who work for them .\n",
      " \n",
      "a gin for me .\n",
      " \n",
      " ireland is to you all .\n",
      " \n",
      " the scottich pleased , the queen ' s bombings in london from persecution .\n",
      " \n",
      "i look at our country .\n",
      " \n",
      "the houses of parliament democracy ' 2 is not only follow these developments more closely than ever .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(lizzie_3gram.Predict(context,maxLength=30,numberOfConsideredWords=50))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d297f69",
   "metadata": {},
   "source": [
    "If we want to use the same model again, we can simply create a new `ToyLM_ngram` object and load it. We can start generating text right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a052b8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the queen is back africa samaritan it have 5 when and our country is immune from these dangers and we see your principles being put into context the invaluable public and voluntary'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lizzi_3gram = lm.ToyLM_ngram()\n",
    "\n",
    "new_lizzi_3gram.Load('models/lizzie_3gram.pickle')\n",
    "\n",
    "new_lizzi_3gram.Predict('the queen is back',maxLength=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca556610",
   "metadata": {},
   "source": [
    "### Queen of England 4-gram model\n",
    "We will now try the same with a 4-gram model this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe428b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "lizzie_4gram = lm.ToyLM_ngram()\n",
    "\n",
    "trainSentences = lizzie_4gram.GetSentences('./materials/HerMajestySpeechesDataset/train.txt')\n",
    "testSentences = lizzie_4gram.GetSentences('./materials/HerMajestySpeechesDataset/test.txt')\n",
    "\n",
    "lizzie_4gram.CreateModel(trainSentences,4)\n",
    "lizzie_4gram.Save('models/lizzie_4gram.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7208c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average perplexity is:  17.745726\n",
      "* Note: The percentage of valid tests is:  12.400072674418606 %\n"
     ]
    }
   ],
   "source": [
    "lizzie_4gram.Test(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53906ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  i to i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      " \n",
      "hello , i am the very i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      " \n",
      "i love the commonwealth i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      " \n",
      "where are you philip i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      " \n",
      " hi there folks i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      " \n",
      "a gin for me i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      " \n",
      " ireland is i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      " \n",
      " the scottich i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      " \n",
      "i look at our country i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      " \n",
      "the houses of parliament , 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "contexts = ['',\n",
    "           'hello, i am the',\n",
    "           'i love the commonwealth',\n",
    "           'where are you philip',\n",
    "           'hi there folks',\n",
    "           'a gin for me',\n",
    "           'ireland is',\n",
    "           'the scottich',\n",
    "           'i look at our country',\n",
    "            'the houses of parliament']\n",
    "\n",
    "for context in contexts:\n",
    "    print(lizzie_4gram.Predict(context,maxLength=30,numberOfConsideredWords=1))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e3bd304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  which bind all world us , is the date of the wedding anniversary of my accession is giving so many people , adults and children , and children to appreciate\n",
      " \n",
      "hello , i am the very , timeless there welcomed at , has are do 2 - who society have have have in and ignored â at festival broadcasting than .\n",
      " \n",
      "i love the commonwealth ' s leaders , as evident in australia last week ; and to share in the ideals of this unique gathering of nations , to celebrate an\n",
      " \n",
      "where are you philip years global reply and .\n",
      " \n",
      " hi there folks them great mark of , westminster laid some region young experience in i everywhere memories proud meeting building even a remarkable effort to the pounds tragedy ,\n",
      " \n",
      "a gin for me to see so many cadets from the commonwealth and to all those who lost their lives , and the commonwealth and around the world , australians are\n",
      " \n",
      " ireland is of grateful the rise determine to family , of friendship , of language and education , of the continuity of our national spirit ; and its symbol will\n",
      " \n",
      " the scottich meet initiative this from land enriched on .\n",
      " \n",
      "i look at our country looking and which of deeds their battle in aids period .\n",
      " \n",
      "the houses of parliament , 3 0 0 april 2 0 0 2 2 .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(lizzie_4gram.Predict(context,maxLength=30))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8539c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  s , i kingdom i , have , , , , for s to , , 0 9 0 been been 4 force s i you wales wales , you\n",
      " \n",
      "hello , i am the very 4 0 gentlemen 2 s queen have gentlemen to as kingdom 9 to 0 0 wales are government gentlemen your been gentlemen the 9 i of\n",
      " \n",
      "i love the commonwealth s 9 wales to 9 4 your 9 2 s queen first for the been wales and 2 9 queen the 2 9 9 march 2 0\n",
      " \n",
      "where are you philip parliament i gentlemen of 2 the are meeting , 2 force have christmas i as 4 , to to 2 century have 0 gentlemen wales .\n",
      " \n",
      " hi there folks to meeting 2 are , , 0 s , of force 4 , thank i 9 wales s as you for have me of thank , 2\n",
      " \n",
      "a gin for me wales to , 2 the gentlemen 9 and to the courage of those who have been affected by events in afghanistan and saddened by the casualties suffered\n",
      " \n",
      " ireland is of to , thank 9 s 0 thank of been of 2 for queen kingdom your 0 s 0 2 you to gentlemen i century scottish s s\n",
      " \n",
      " the scottich 5 century 9 , 2 me , for 2 the s been your 2 i and and , , 9 wales 9 4 i 2 to gentlemen been\n",
      " \n",
      "i look at our country wales , century kingdom i 2 2 government 4 2 parliament kingdom gentlemen as to 9 2 2 to help me put my own worries into perspective\n",
      " \n",
      "the houses of parliament , 3 0 0 you and your wife will be celebrating your own golden wedding .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(lizzie_4gram.Predict(context,maxLength=30,numberOfConsideredWords=50))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a92f2",
   "metadata": {},
   "source": [
    "### James Joyce 3-gram model\n",
    "\n",
    "We will now hop over the other side of the Irish sea, and create a model that speaks like James Joyce by training on Dubliners (available at: https://www.gutenberg.org/ebooks/2814).  \n",
    "After downloading the plain text version and saving as `dubliners.txt`, and extracting the sentences, we should create our own train, and test sets; assigning 80% of the sentences to the train set, and 20% to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51f17141",
   "metadata": {},
   "outputs": [],
   "source": [
    "joyce_3gram = lm.ToyLM_ngram()\n",
    "\n",
    "sentences = joyce_3gram.GetSentences('./materials/dubliners.txt')\n",
    "sentences = [elem for elem in sentences if elem.strip() != ''] # Eliminate empty elements\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(sentences)\n",
    "\n",
    "trainSentences = sentences[:int(len(sentences)*0.8)]\n",
    "testSentences = sentences[int(len(sentences)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98115701",
   "metadata": {},
   "source": [
    "Once that is done, it is business as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b73f213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average perplexity is:  9.016742\n",
      "* Note: The percentage of valid tests is:  24.222155418608146 %\n"
     ]
    }
   ],
   "source": [
    "joyce_3gram.CreateModel(trainSentences,3)\n",
    "joyce_3gram.Save('models/joyce_3gram.pickle')\n",
    "\n",
    "joyce_3gram.Test(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1443118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mr â mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr\n",
      " \n",
      "hello , i am the mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr\n",
      " \n",
      "i love the dublin musical world .â mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr\n",
      " \n",
      "where are you father flynn .â mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr\n",
      " \n",
      "hi there folks mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr\n",
      " \n",
      "a ginger beer for me ,â said mr cunningham .\n",
      " \n",
      " ireland is mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr\n",
      " \n",
      " the english â mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr\n",
      " \n",
      "the laws of the country .\n",
      " \n",
      "they walked along the shaft of grey light : mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr mr\n",
      " \n"
     ]
    }
   ],
   "source": [
    "contexts = ['',\n",
    "           'hello, i am the',\n",
    "           'i love the dublin',\n",
    "           'where are you father flynn',\n",
    "           'hi there folks',\n",
    "           'a ginger beer for me',\n",
    "           'ireland is',\n",
    "           'the english',\n",
    "           'the laws of the country',\n",
    "            'they walked along']\n",
    "\n",
    "for context in contexts:\n",
    "    print(joyce_3gram.Predict(context,maxLength=30,numberOfConsideredWords=1))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf0a1bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  whom paid the driver of the project gutenberg - tm , to make an .\n",
      " \n",
      "hello , i am the the both the you to break out again a few minutes the women out of the car .\n",
      " \n",
      "i love the dublin musical world .â accepted ivors of men and went towards her nephew â s eve .\n",
      " \n",
      "where are you father flynn .â and ever the woe she a later the the front of her .â purposed waiting to be thought that in her eyes he would never see again\n",
      " \n",
      "hi there folks joy old the himself world and was about to knit his brows and , when she spoke to me , gabriel himself had taken up beyond the river\n",
      " \n",
      "a ginger beer for me .\n",
      " \n",
      " ireland is said music henchy !â three to in the united states and three ladies , with a soft wet substance with her ) do anxiously studied project .\n",
      " \n",
      " the english â , has many to she .\n",
      " \n",
      "the laws of the country where you are , crofton ?â cried mr kernan sensibly , what an end to herself , her aunts what or show the difference between us ,â said\n",
      " \n",
      "they walked along the route , and a minute or so in the eclogues , he threw open a window significantly .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(joyce_3gram.Predict(context,maxLength=30))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ae4a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  aunt ,â the , mr conroy ?â asked little chandler , â each in his own , insisting on the following sentence , but covertly proud of their friends ,\n",
      " \n",
      "hello , i am the a s â the connor â t arcy mr mr the ll s â mr had had the night air .\n",
      " \n",
      "i love the dublin musical world .â s been drinking since friday .â a know - s mr browne got into a society , mr conroy ?â asked mr o â clock\n",
      " \n",
      "where are you father flynn .â tm mr a , tm kate â s a stroke cunningham e â was a decent sort so long as anyone could remember the way you chaps\n",
      " \n",
      "hi there folks a â m right ,â said mr o â clock from mr ryan .\n",
      " \n",
      "a ginger beer for me ,â he said coldly .\n",
      " \n",
      " ireland is â tm said a i connor â cunningham tm was s was â ou frightened , love ?... there mr connor â t you remind him ?â said\n",
      " \n",
      " the english â cunningham arcy had s was man , who was regarding us with offers to donate royalties under this agreement , the big hat who had come and\n",
      " \n",
      "the laws of the country .\n",
      " \n",
      "they walked along nassau street and walked up to dublin and holland ; and , carrying them to the man â s , and thanks ever so much the better for\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(joyce_3gram.Predict(context,maxLength=30,numberOfConsideredWords=50))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e7aad",
   "metadata": {},
   "source": [
    "## LSTM models\n",
    "\n",
    "### Queen of England 4-size window LSTM model\n",
    "\n",
    "To work with an LSTM model, we first need to create a `ToyLM_LSTM` object, and extract the train, validation and test sets as we have done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "846827cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lizzie_4LSTM = lm.ToyLM_LSTM()\n",
    "\n",
    "trainSentences = lizzie_4LSTM.GetSentences('./materials/HerMajestySpeechesDataset/train.txt')\n",
    "valSentences = lizzie_4LSTM.GetSentences('./materials/HerMajestySpeechesDataset/dev.txt')\n",
    "testSentences = lizzie_4LSTM.GetSentences('./materials/HerMajestySpeechesDataset/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ece5d",
   "metadata": {},
   "source": [
    "Next we need to process the sentences using the `ProcessData` method. By default this method creates windows of size 4. We also need to set `trainSet` to `True` when processing the corresponding set, as the method will construct the tokenizer only with the test set. When set to `True`, the tokenizer will not only be created but also saved, so we need to specify a path for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e151dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, yTrain = lizzie_4LSTM.ProcessData(trainSentences,trainSet=True,tokenizerPath='models/lizzie_4LSTM.pickle')\n",
    "xVal, yVal = lizzie_4LSTM.ProcessData(valSentences)\n",
    "xTest, yTest = lizzie_4LSTM.ProcessData(testSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3fbdd",
   "metadata": {},
   "source": [
    "We can now create the model. By default the embedding size is calculated using an heuristic, 64 LSTM units are used, and the dropout and recurrent dropout rates are set to 0.2 to avoid over fitting. Moreover the a model summary is not printed, and the vocabulary size is set to the total number of tokens in the corpus. Any of these parameters can be modified though.  \n",
    "\n",
    "The `Train` method should receive the train features and labels, but can receive additional parameters like a validation tuple, number of epochs, optimizer, metrics, batch size, and metrics.  \n",
    "Because an early-stopping callback has been set for the training loop so that only the best model (validation loss-wise) is saved, we need to specify a path to save the model if a validation tuple was provided and `saveBest` was not set to `False`. Related to this, the `patience` parameter is set to 5 by default, but it can be modified. The training process is halted when the validation loss has not improved in the number of cycles specified by this parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "150b224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 4, 116)            612828    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                46336     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5283)              343395    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,002,559\n",
      "Trainable params: 1,002,559\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 20:37:12.976474: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-04 20:37:14.114653: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1882/1882 [==============================] - ETA: 0s - loss: 6.4348 - perplexity: 623.1867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 20:39:20.566546: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1882/1882 [==============================] - 132s 69ms/step - loss: 6.4348 - perplexity: 623.1867 - val_loss: 6.3460 - val_perplexity: 570.2281\n",
      "Epoch 2/50\n",
      "1882/1882 [==============================] - 130s 69ms/step - loss: 5.7826 - perplexity: 324.6066 - val_loss: 6.0719 - val_perplexity: 433.4855\n",
      "Epoch 3/50\n",
      "1882/1882 [==============================] - 131s 70ms/step - loss: 5.3862 - perplexity: 218.3620 - val_loss: 5.9514 - val_perplexity: 384.2718\n",
      "Epoch 4/50\n",
      "1882/1882 [==============================] - 129s 69ms/step - loss: 5.0826 - perplexity: 161.1992 - val_loss: 5.9102 - val_perplexity: 368.7772\n",
      "Epoch 5/50\n",
      "1882/1882 [==============================] - 129s 69ms/step - loss: 4.8466 - perplexity: 127.3014 - val_loss: 5.9296 - val_perplexity: 375.9964\n",
      "Epoch 6/50\n",
      "1882/1882 [==============================] - 144s 77ms/step - loss: 4.6475 - perplexity: 104.3214 - val_loss: 5.9479 - val_perplexity: 382.9339\n",
      "Epoch 7/50\n",
      "1882/1882 [==============================] - 143s 76ms/step - loss: 4.4697 - perplexity: 87.3300 - val_loss: 6.0086 - val_perplexity: 406.9293\n",
      "Epoch 8/50\n",
      "1882/1882 [==============================] - 142s 76ms/step - loss: 4.3088 - perplexity: 74.3536 - val_loss: 6.0312 - val_perplexity: 416.2235\n",
      "Epoch 9/50\n",
      "1882/1882 [==============================] - 141s 75ms/step - loss: 4.1654 - perplexity: 64.4205 - val_loss: 6.0874 - val_perplexity: 440.2962\n"
     ]
    }
   ],
   "source": [
    "lizzie_4LSTM.CreateModel(printSummary=True)\n",
    "lizzie_4LSTM.Train(xTrain, yTrain, valTuple=(xVal, yVal), epochs=50, modelName='models/lizzie_4LSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96524c",
   "metadata": {},
   "source": [
    "Next we calculate the perplexity by calling `Test`. It is important to mention that these LSTM models do not suffer from the aforementioned problems regarding the calculation of perplexity as n-gram models do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b64b453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average perplexity is:  359.27008056640625\n"
     ]
    }
   ],
   "source": [
    "lizzie_4LSTM.Test(xTest,yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1845c",
   "metadata": {},
   "source": [
    "We can finally generate texts, first grabbing the most probable word (i.e. setting the number of considered words to 1). Heads-up, she really seems to like the commonwealth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc64ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 20:57:38.687546: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-04 20:57:38.912399: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am pleased to be able to be able to be able to be able to be able to be able to be able to be able to be able\n",
      " \n",
      "hello, i am the pleased to be able to be able to be able to be able to be able to be able to be able to be able to be able to be\n",
      " \n",
      "i love the commonwealth commonwealth , and i am pleased to be able to be able to be able to be able to be able to be able to be able to be able\n",
      " \n",
      "where are you philip have been able to be in the commonwealth , and i am pleased to be able to be able to be able to be able to be able to be\n",
      " \n",
      "hi there folks is a great pleasure for the commonwealth , and i am pleased to be able to be able to be able to be able to be able to be able\n",
      " \n",
      "a gin for me the commonwealth , and i am pleased to be able to be able to be able to be able to be able to be able to be able to be\n",
      " \n",
      "ireland is premier , i am pleased to be able to be able to be able to be able to be able to be able to be able to be able to\n",
      " \n",
      "the scottich queen ' s commonwealth day message , the commonwealth is a great pleasure for the commonwealth , and i am pleased to be able to be able to be able\n",
      " \n",
      "i look at our country own history , and to the commonwealth , and i am pleased to be able to be able to be able to be able to be able to be able\n",
      " \n",
      "the houses of parliament the commonwealth , and i am pleased to be able to be able to be able to be able to be able to be able to be able to be\n",
      " \n"
     ]
    }
   ],
   "source": [
    "contexts = ['',\n",
    "           'hello, i am the',\n",
    "           'i love the commonwealth',\n",
    "           'where are you philip',\n",
    "           'hi there folks',\n",
    "           'a gin for me',\n",
    "           'ireland is',\n",
    "           'the scottich',\n",
    "           'i look at our country',\n",
    "            'the houses of parliament']\n",
    "\n",
    "for context in contexts:\n",
    "    print(lizzie_4LSTM.Predict(context,maxLength=30,numberOfConsideredWords=1))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0da3a",
   "metadata": {},
   "source": [
    "Next grabbing any word based on a probability distribution (which is the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f6065a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edinburgh was a strong industry of vice democracy , elsewhere , dynamic commitment and work down that we are turn forward on the frauenkirche overseas in education , the economic\n",
      " \n",
      "hello, i am the to know that your newspapers is a new canadian mounted illness .\n",
      " \n",
      "i love the commonwealth w .\n",
      " \n",
      "where are you philip share with their famous singer .\n",
      " \n",
      "hi there folks .\n",
      " \n",
      "a gin for me life was my grandchildren because of your lord - and staying .\n",
      " \n",
      "ireland is drives this year will county anxiously - neither regiment we saw , it in canada , general 5 .\n",
      " \n",
      "the scottich course of your two twentieth time also then that are serving up and renewal .\n",
      " \n",
      "i look at our country two and welsh opposition of your national assembly or in the generosity and admirers of overseas has not done many of the air world represented ; what today has been\n",
      " \n",
      "the houses of parliament my creative nations are intensely wanted in my families , in london and bewilderment , i would particularly be for struck all how your election is enrich as i am\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(lizzie_4LSTM.Predict(context,maxLength=30))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a179c4",
   "metadata": {},
   "source": [
    "And finally, again based in a probability distribution, but taking into account only the 50 most probable tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9e60c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is so many hundred years for australia , i know it is no difficult for we celebrate in that times - of new ways such the future .\n",
      " \n",
      "hello, i am the speaking to that there are no opportunity to be together in the wider world .\n",
      " \n",
      "i love the commonwealth year i have travelled back over the world , is as a toast to me their own energy , and care and your family come here , as a deep\n",
      " \n",
      "where are you philip are often about doing all their best - bearers to a rich city whose prairies it , as they are very best to play and those who have died before\n",
      " \n",
      "hi there folks are great history , and is to the warmth and you are able to offer around people in both - a time to achieve a pleasure for our good ,\n",
      " \n",
      "a gin for me the nation at the new city , and those from its spirit .\n",
      " \n",
      "ireland is premier and i remember the commonwealth , and the past is now only to help the commonwealth , and i have continued so many year will be a time of\n",
      " \n",
      "the scottich queen and , my country and the united kingdom have taken one of our time to celebrate the way .\n",
      " \n",
      "i look at our country young people , for the globe .\n",
      " \n",
      "the houses of parliament our country and the first century , but we have the united kingdom , and me to take our way , more as so many of a great village .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(lizzie_4LSTM.Predict(context,maxLength=30,numberOfConsideredWords=50))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55ad5b",
   "metadata": {},
   "source": [
    "As with the n-gram models, we can load the model. This time however, we need to also specify the path for the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e0abb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 20:57:59.798127: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-04 20:58:00.007270: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the queen is back to the commonwealth , and all have already only too much to find british ways our reputation to meet us on and us in a very special assembly for these'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lizzie_4LSTM = lm.ToyLM_LSTM()\n",
    "\n",
    "new_lizzie_4LSTM.Load('models/lizzie_4LSTM.h5','models/lizzie_4LSTM.pickle')\n",
    "\n",
    "new_lizzie_4LSTM.Predict('the queen is back',maxLength=30,numberOfConsideredWords=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2bcd5",
   "metadata": {},
   "source": [
    "### Queen of England 6-size window LSTM model\n",
    "\n",
    "We will repeat the same procedure but this time using a windows of size 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d573c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 4, 116)            612828    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                46336     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5283)              343395    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,002,559\n",
      "Trainable params: 1,002,559\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 20:58:02.294337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1882/1882 [==============================] - ETA: 0s - loss: 6.2546 - perplexity: 487.3382"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 21:00:19.460720: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1882/1882 [==============================] - 142s 75ms/step - loss: 6.2546 - perplexity: 487.3382 - val_loss: 6.1330 - val_perplexity: 460.8027\n",
      "Epoch 2/50\n",
      "1882/1882 [==============================] - 144s 77ms/step - loss: 5.5435 - perplexity: 255.5735 - val_loss: 5.8781 - val_perplexity: 357.1156\n",
      "Epoch 3/50\n",
      "1882/1882 [==============================] - 145s 77ms/step - loss: 5.1397 - perplexity: 170.6643 - val_loss: 5.8421 - val_perplexity: 344.4870\n",
      "Epoch 4/50\n",
      "1882/1882 [==============================] - 143s 76ms/step - loss: 4.8478 - perplexity: 127.4548 - val_loss: 5.8852 - val_perplexity: 359.6878\n",
      "Epoch 5/50\n",
      "1882/1882 [==============================] - 135s 72ms/step - loss: 4.6144 - perplexity: 100.9292 - val_loss: 5.9050 - val_perplexity: 366.8697\n",
      "Epoch 6/50\n",
      "1882/1882 [==============================] - 135s 72ms/step - loss: 4.4173 - perplexity: 82.8721 - val_loss: 5.9648 - val_perplexity: 389.4742\n",
      "Epoch 7/50\n",
      "1882/1882 [==============================] - 135s 72ms/step - loss: 4.2478 - perplexity: 69.9515 - val_loss: 6.0328 - val_perplexity: 416.8806\n",
      "Epoch 8/50\n",
      "1882/1882 [==============================] - 134s 71ms/step - loss: 4.0918 - perplexity: 59.8483 - val_loss: 6.0847 - val_perplexity: 439.1051\n"
     ]
    }
   ],
   "source": [
    "lizzie_6LSTM = lizzie = lm.ToyLM_LSTM()\n",
    "\n",
    "trainSentences = lizzie_6LSTM.GetSentences('./materials/HerMajestySpeechesDataset/train.txt')\n",
    "valSentences = lizzie_6LSTM.GetSentences('./materials/HerMajestySpeechesDataset/dev.txt')\n",
    "testSentences = lizzie_6LSTM.GetSentences('./materials/HerMajestySpeechesDataset/test.txt')\n",
    "\n",
    "xTrain, yTrain = lizzie_6LSTM.ProcessData(trainSentences,trainSet=True,tokenizerPath='models/lizzie_6LSTM.pickle')\n",
    "xVal, yVal = lizzie_6LSTM.ProcessData(valSentences)\n",
    "xTest, yTest = lizzie_6LSTM.ProcessData(testSentences)\n",
    "\n",
    "lizzie_6LSTM.CreateModel(printSummary=True)\n",
    "lizzie_6LSTM.Train(xTrain, yTrain, valTuple=(xVal, yVal), epochs=50, modelName='models/lizzie_6LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2c1a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average perplexity is:  349.2349853515625\n"
     ]
    }
   ],
   "source": [
    "lizzie_6LSTM.Test(xTest,yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dddd712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 21:16:39.200176: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-04 21:16:39.410896: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am pleased to be a place to be able to be able to be able to be able to be able to be able to be able to be\n",
      " \n",
      "hello, i am the pleased to be a place to be able to be able to be able to be able to be able to be able to be able to be able to\n",
      " \n",
      "i love the commonwealth commonwealth , and the queen ' s commonwealth day message , 2 0 0 2 2 , i have been struck by the queen ' s commonwealth day message ,\n",
      " \n",
      "where are you philip have been able to be able to be able to be able to be able to be able to be able to be able to be able to be able\n",
      " \n",
      "hi there folks is a pleasure to be able to be able to be able to be able to be able to be able to be able to be able to be able\n",
      " \n",
      "a gin for me the world .\n",
      " \n",
      "ireland is , the queen ' s commonwealth day message , 2 0 0 2 2 , i have been struck by the queen ' s commonwealth day message , 2 0\n",
      " \n",
      "the scottich queen ' s commonwealth day message , 2 0 0 2 2 , i have been struck by the queen ' s commonwealth day message , 2 0 0 2\n",
      " \n",
      "i look at our country own lives .\n",
      " \n",
      "the houses of parliament the commonwealth , and the queen ' s commonwealth day message , 2 0 0 2 2 , i have been struck by the queen ' s commonwealth day message\n",
      " \n"
     ]
    }
   ],
   "source": [
    "contexts = ['',\n",
    "           'hello, i am the',\n",
    "           'i love the commonwealth',\n",
    "           'where are you philip',\n",
    "           'hi there folks',\n",
    "           'a gin for me',\n",
    "           'ireland is',\n",
    "           'the scottich',\n",
    "           'i look at our country',\n",
    "            'the houses of parliament']\n",
    "\n",
    "for context in contexts:\n",
    "    print(lizzie_6LSTM.Predict(context,maxLength=30,numberOfConsideredWords=1))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4565832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep atlantic stage .\n",
      " \n",
      "hello, i am the delighted to be back made before on gone in london , and down our life on the royal jubilee horribilis over the birth fields .\n",
      " \n",
      "i love the commonwealth old play given us they all here today .\n",
      " \n",
      "where are you philip are very been here in â, during the year like developing its whilst energy to western support .\n",
      " \n",
      "hi there folks is no doubt that all those and theory .\n",
      " \n",
      "a gin for me the world , service to do just with our two ways with urgent glorious levels at the highly twentieth century , i would like for these and british powers to\n",
      " \n",
      "ireland is , the we draw across your communities , and welcoming meetings of the peace will emerge for the rock of carnivals throughout the years .\n",
      " \n",
      "the scottich scottish parliament , 3 3 .\n",
      " \n",
      "i look at our country first countries , whose guests has blessed on become friendship and bear .\n",
      " \n",
      "the houses of parliament my two and recent freedom between 1 9 4 4 4 , was loved , 2 0 0 , runs , for your peace .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(lizzie_6LSTM.Predict(context,maxLength=30))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea81dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last year we have been encouraged by the first century .\n",
      " \n",
      "hello, i am the delighted to the more fundamental century .\n",
      " \n",
      "i love the commonwealth more fortune that the scottish parliament â the opening of this great advances of london , has been doing only at the new welcome state years , and my present\n",
      " \n",
      "where are you philip , mr .\n",
      " \n",
      "hi there folks and the first who will find to make british investment to see , to their accession , but all too much for us , for me and a shared future\n",
      " \n",
      "a gin for me many people in this day .\n",
      " \n",
      "ireland is will be a reminder of us all .\n",
      " \n",
      "the scottich people of this country ' s message as a strong in great care and prosperity to ireland to the netherlands , our last fifty years ago .\n",
      " \n",
      "i look at our country young and old , in a new building to the future , you continue to celebrate the world , and i shall be seen , i would like to mark\n",
      " \n",
      "the houses of parliament the commonwealth , but not me for so much to the benefit of the challenges .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(lizzie_6LSTM.Predict(context,maxLength=30,numberOfConsideredWords=50))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918d922",
   "metadata": {},
   "source": [
    "### James Joyce 6-size window LSTM model\n",
    "\n",
    "And finally, it is now time to train an LSTM models with Dubliners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ac890e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 4, 127)            806450    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                49152     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6350)              412750    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,268,352\n",
      "Trainable params: 1,268,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 00:27:34.275815: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1860/1860 [==============================] - ETA: 0s - loss: 6.3780 - perplexity: 7542.1055"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 00:29:42.692154: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1860/1860 [==============================] - 135s 71ms/step - loss: 6.3780 - perplexity: 7542.1055 - val_loss: 6.0264 - val_perplexity: 414.2220\n",
      "Epoch 2/50\n",
      "1860/1860 [==============================] - 132s 71ms/step - loss: 5.6212 - perplexity: 276.2148 - val_loss: 5.7414 - val_perplexity: 311.5085\n",
      "Epoch 3/50\n",
      "1860/1860 [==============================] - 130s 70ms/step - loss: 5.2211 - perplexity: 185.1331 - val_loss: 5.6876 - val_perplexity: 295.1831\n",
      "Epoch 4/50\n",
      "1860/1860 [==============================] - 131s 70ms/step - loss: 4.9345 - perplexity: 138.9987 - val_loss: 5.6905 - val_perplexity: 296.0366\n",
      "Epoch 5/50\n",
      "1860/1860 [==============================] - 132s 71ms/step - loss: 4.7054 - perplexity: 110.5461 - val_loss: 5.7532 - val_perplexity: 315.1913\n",
      "Epoch 6/50\n",
      "1860/1860 [==============================] - 132s 71ms/step - loss: 4.5088 - perplexity: 90.8087 - val_loss: 5.7854 - val_perplexity: 325.5006\n",
      "Epoch 7/50\n",
      "1860/1860 [==============================] - 131s 70ms/step - loss: 4.3374 - perplexity: 76.5112 - val_loss: 5.8583 - val_perplexity: 350.1316\n",
      "Epoch 8/50\n",
      "1860/1860 [==============================] - 132s 71ms/step - loss: 4.1834 - perplexity: 65.5912 - val_loss: 5.8999 - val_perplexity: 365.0103\n"
     ]
    }
   ],
   "source": [
    "joyce_6LSTM = lizzie = lm.ToyLM_LSTM()\n",
    "\n",
    "sentences = joyce_6LSTM.GetSentences('./materials/dubliners.txt')\n",
    "sentences = [elem for elem in sentences if elem.strip() != ''] # Eliminate empty elements\n",
    "\n",
    "import random\n",
    "random.shuffle(sentences)\n",
    "\n",
    "trainSentences = sentences[:int(len(sentences)*0.7)]\n",
    "valSentences = sentences[int(len(sentences)*0.7):int(len(sentences)*0.9)]\n",
    "testSentences = sentences[:int(len(sentences)*0.9)]\n",
    "\n",
    "xTrain, yTrain = joyce_6LSTM.ProcessData(trainSentences,trainSet=True,tokenizerPath='models/joyce_6LSTM.pickle')\n",
    "xVal, yVal = joyce_6LSTM.ProcessData(valSentences)\n",
    "xTest, yTest = lizzie_6LSTM.ProcessData(testSentences)\n",
    "\n",
    "joyce_6LSTM.CreateModel(printSummary=True)\n",
    "joyce_6LSTM.Train(xTrain, yTrain, valTuple=(xVal, yVal), epochs=50, modelName='models/joyce_6LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d14173bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average perplexity is:  43428.453125\n"
     ]
    }
   ],
   "source": [
    "joyce_6LSTM.Test(xTest,yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b20a6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 00:45:29.203105: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-05 00:45:29.449740: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â i â m not going to be a little man .\n",
      " \n",
      "hello, i am the not much at him .\n",
      " \n",
      "i love the dublin registered and , as if he had been a bit of the room .\n",
      " \n",
      "where are you father flynn ?â said mr o â connor .\n",
      " \n",
      "hi there folks was a great sum with him .\n",
      " \n",
      "a ginger beer for me the room .\n",
      " \n",
      "ireland is of the room .\n",
      " \n",
      "the english man had grown out of the room .\n",
      " \n",
      "the laws of the country room .\n",
      " \n",
      "they walked along into the room .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "contexts = ['',\n",
    "           'hello, i am the',\n",
    "           'i love the dublin',\n",
    "           'where are you father flynn',\n",
    "           'hi there folks',\n",
    "           'a ginger beer for me',\n",
    "           'ireland is',\n",
    "           'the english',\n",
    "           'the laws of the country',\n",
    "            'they walked along']\n",
    "\n",
    "for context in contexts:\n",
    "    print(joyce_6LSTM.Predict(context,maxLength=30,numberOfConsideredWords=1))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa6c25a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry pride in which pushed back and told me for a young body was joy into clouds .\n",
      " \n",
      "hello, i am the not is signs to him .\n",
      " \n",
      "i love the dublin chief concert .\n",
      " \n",
      "where are you father flynn heard .\n",
      " \n",
      "hi there folks he received a rebuke hat and stood two bodies dog or far or set on dublin but joe is standing at the fire at him preserve and read .\n",
      " \n",
      "a ginger beer for me him in all together .\n",
      " \n",
      "ireland is german .\n",
      " \n",
      "the english lady waited in the window .\n",
      " \n",
      "the laws of the country house reward it won â t indeed ,â she wrote never always like his own right in the lion , because he say that , as he might kathleen once\n",
      " \n",
      "they walked along again on the victim - sliding .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(joyce_6LSTM.Predict(context,maxLength=30))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f1773cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â yes !â said the man was to say that the way .\n",
      " \n",
      "hello, i am the ?â _ mr o â connor again .\n",
      " \n",
      "i love the dublin prelude .â but the car , shining with the river .\n",
      " \n",
      "where are you father flynn mayor , take all the night of mrs mooney , â it â s daughter .â when the invalid of aughrim_ : _ â i â m my copy ,\n",
      " \n",
      "hi there folks were and called from up and corley â s head .\n",
      " \n",
      "a ginger beer for me a moment â s works .\n",
      " \n",
      "ireland is they was in the counter and seemed to take the window in the man , which she gave their pen or in a moment from her out of a dairyman\n",
      " \n",
      "the english hall - chair while the room .\n",
      " \n",
      "the laws of the country street .\n",
      " \n",
      "they walked along as after his companions , including getting out in the other door .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for context in contexts:\n",
    "    print(joyce_6LSTM.Predict(context,maxLength=30,numberOfConsideredWords=50))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39098742",
   "metadata": {},
   "source": [
    "## Conclusions \n",
    "\n",
    "Although of course the quality of the n-gram and LSTM-based toy models in not comparable to a state the art large language model trained on a huge corpus, and using supervised and reinforcement learning like chatGPT, they at least do quite a good job at capturing the writing style of the set they where trained on.  \n",
    "They also managed to produce some fully coherent (albeit short) phrases like `the english lady waited in the window .` (James Joyce 6-size window LSTM model, with 50 words considered).  \n",
    "A few longer phrases are also relatively coherent, for example: `a gin for me to see so many cadets from the commonwealth and to all those who lost their lives , and the commonwealth and around the world , australians are`, taken from the queen of England 4-gram model, and considering all words.  \n",
    "The models also managed to capture or learn (in the case of LSTM models) things like digits generally appearing one after another (as in `i love the commonwealth - canada , 1 1 march 2 0 0 2 2 is not a year of the diversity that has necessarily kept people apart has , quite simply`, for the queen of England 3-gram model considering all words in the vocabulary), or to add question marks when prompted with a context structured like a question; for example, when prompted with `where are you father flynn` the James Joyce 6-size window LSTM model with all word considered, produced: `where are you father flynn ?â said mr o â connor .`   \n",
    "All models produced poor results very lacking in creativity when the taking only the most probable token for generation. Results got much better when picking among the 50 most probable, or from all tokens within the vocabulary.  \n",
    "All in all the models produced some interesting phrases like `ireland is german .`, `the scottich pleased , the queen ' s bombings in london from persecution .` or `they walked along nassau street and walked up to dublin and holland ; and , carrying them to the man â s , and thanks ever so much the better for`. \n",
    "\n",
    "Regarding the differences between the n-gram and LSTM models, the n-gram models are very quick to create as, unlike LSTMs, they do not require training. On the other hand, neural network-based models have the potential to capture much more sophisticated language patterns.   \n",
    "Perplexity-wise, when this metric could actually be computed, n-gram models scored significantly better than LSTMs ones. As mentioned before however, the n-gram models are completely perplexed a good amount of the times. The first n-gram model constructed in the notebook, for example, got a perplexity of around 8 for 25% of the tests, while being utterly perplexed for around 75% of the tests. In contrast, the LSTM model scored an average perplexity of around 350 across the board; significantly higher, but more consistent.  \n",
    "The James Joyce 6-size window LSTM model scored by far the highest perplexity of close to 43,500. This could be explained by its significantly bigger vocabulary size when compared to that of the queen's models. Despite this, the model managed to create results of comparable quality to other models, highlighting the importance of not taking perplexity as the sole metric for judging the performance of a language model. It should be regarded contextually, taking into account the inherit complexity of the corpus and model.  \n",
    "\n",
    "Qualitatively, all models produced results of similar caliber, and it would be hard to tell with which model trained on the same corpus a particular sentence was created. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
